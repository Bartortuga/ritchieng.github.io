<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title></title>
        <description>I am Ritchie Ng, a machine learning engineer specializing in deep learning and computer vision. Check out my code guides and keep ritching for the skies!</description>
        <link>http://www.ritchieng.com/</link>
        <atom:link href="http://www.ritchieng.com/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Mon, 10 Apr 2017 19:50:04 +0800</pubDate>
        <lastBuildDate>Mon, 10 Apr 2017 19:50:04 +0800</lastBuildDate>
        <generator>Jekyll v3.0.3</generator>
        
        <item>
            <title>Comic Series: Plain Vanilla</title>
            <description>&lt;p&gt;This is just a plain vanilla post.
&lt;br /&gt;&lt;img src=&quot;https://res.cloudinary.com/ritchieng/image/upload/v1490607597/vanilla_27_3_2017_wvjeax.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
            <pubDate>Mon, 27 Mar 2017 00:00:00 +0800</pubDate>
            <link>http://www.ritchieng.com/comic-series/plain-vanilla</link>
            <guid isPermaLink="true">http://www.ritchieng.com/comic-series/plain-vanilla</guid>
            
            <category>comic_series</category>
            
            
        </item>
        
        <item>
            <title>REWORK Deep Learning Summit Singapore</title>
            <description>&lt;p&gt;I’ll be presenting at &lt;a href=&quot;https://www.re-work.co/events/deep-learning-summit-singapore-april-2017&quot;&gt;REWORK Deep Learning Summit Singapore&lt;/a&gt; on 27-28 April 2017!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;
&lt;br /&gt;
With every deep learning algorithm comes a set of hyperparameters. Optimizing them is crucial in achieving faster convergence and lower error rates. For many years, the majority of people in the deep learning community has been using common heuristics to tune hyperparameters such as learning rates, decay rates and L2 regularization. In recent works, researchers have attempted to cast hyperparameter optimization as a deep learning problem but they are limited by their lack of scalability. I show how it is now possible for scalable hyperparameter optimization that accelerates convergence that can be trained on one problem while enjoying the benefits of transfer learning that is scalable. This has impact on the industrial level where deep learning algorithms can be accelerated to convergence without manual hand-tuning even for large models.&lt;/p&gt;
</description>
            <pubDate>Mon, 13 Mar 2017 00:00:00 +0800</pubDate>
            <link>http://www.ritchieng.com/conferences/rework-deep-learning-summit-singapore-2017</link>
            <guid isPermaLink="true">http://www.ritchieng.com/conferences/rework-deep-learning-summit-singapore-2017</guid>
            
            <category>news</category>
            
            <category>machine_learning</category>
            
            
        </item>
        
        <item>
            <title>Gloqo: Search for code for research papers on arXiv</title>
            <description>&lt;p&gt;This is basically “the Google” for searching code for computer science papers on arXiv. It’s currently concentrated on deep learning and machine learning. I started this companion website to arXiv.org to encourage reproducible research. You can easily search for code that are implemented by the authors themselves or often by others.&lt;/p&gt;

&lt;p&gt;As an undergraduate conducting research in deep learning, I actively try to verify past papers’ experiments before building on them for major projects and papers. And I found that many, if not most, authors do not release their codes. And there are many issues associated with research that is not reproducible as shown in &lt;a href=&quot;http://www.nature.com/news/reproducibility-1.17552&quot;&gt;Nature Reproducible Research&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And because I ran out of ways to extend this website well that is proudly served through Github by Github Pages for free forever, I decided to create a separate application. If you understand how Github Pages only allow static pages, you would come to realize why I could not effectively build this function on this website; or at least effectively build one where others can post too. It’s meant for myself, but I hope some people would find gloqo useful too and contribute!&lt;/p&gt;

&lt;p&gt;I will keep it short and just repeat the &lt;a href=&quot;http://sciencecodemanifesto.org&quot;&gt;Science Code Manifesto&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;All source code written specifically to process data for a published paper must be available to the reviewers and readers of the paper.&lt;/strong&gt;&lt;/p&gt;
</description>
            <pubDate>Fri, 18 Nov 2016 00:00:00 +0800</pubDate>
            <link>http://www.ritchieng.com/gloqo/arxiv-like-code</link>
            <guid isPermaLink="true">http://www.ritchieng.com/gloqo/arxiv-like-code</guid>
            
            <category>news</category>
            
            <category>machine_learning</category>
            
            
        </item>
        
        <item>
            <title>Comic Series: The Future of Labelled Data</title>
            <description>&lt;p&gt;Weirdly, there is something similar offered by &lt;a href=&quot;https://www.scaleapi.com&quot;&gt;Scale&lt;/a&gt;
&lt;br /&gt;&lt;img src=&quot;https://res.cloudinary.com/ritchieng/image/upload/v1489375618/labels_ritchie_14_10_2016_hiwspz.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
            <pubDate>Fri, 14 Oct 2016 00:00:00 +0800</pubDate>
            <link>http://www.ritchieng.com/comic-series/labels</link>
            <guid isPermaLink="true">http://www.ritchieng.com/comic-series/labels</guid>
            
            <category>comic_series</category>
            
            
        </item>
        
        <item>
            <title>Comic Series: Support Vector Machines</title>
            <description>&lt;p&gt;Support Vector Machines are often called Largest Margin Classifier.
&lt;br /&gt;&lt;img src=&quot;https://res.cloudinary.com/ritchieng/image/upload/v1489375612/svm_ritchie_30_09_2016_q2sxkj.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
            <pubDate>Wed, 05 Oct 2016 00:00:00 +0800</pubDate>
            <link>http://www.ritchieng.com/comic-series/svms</link>
            <guid isPermaLink="true">http://www.ritchieng.com/comic-series/svms</guid>
            
            <category>comic_series</category>
            
            
        </item>
        
        <item>
            <title>Comic Series: Convolutional Neural Nets</title>
            <description>&lt;p&gt;As a disclaimer, Derpina intentionally set the output to two classes: “pretty girl” or “not so pretty girl”. We do not know if she intentionally manipulated the labels that led to her findings.
&lt;br /&gt;&lt;img src=&quot;https://res.cloudinary.com/ritchieng/image/upload/v1489375614/convnets_ritchie_24_09_2016_mnp9ue.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
            <pubDate>Fri, 23 Sep 2016 00:00:00 +0800</pubDate>
            <link>http://www.ritchieng.com/comic-series/convnet-derpina/</link>
            <guid isPermaLink="true">http://www.ritchieng.com/comic-series/convnet-derpina/</guid>
            
            <category>comic_series</category>
            
            
        </item>
        
        <item>
            <title>Comic Series: Markov Decision Processes with High Rewards</title>
            <description>&lt;p&gt;Imagine yourself in a grid-like world with two end goals (absorbing states), what would you do? Would you reach for the drumstick or drop into the hole? 
&lt;br /&gt;&lt;img src=&quot;https://res.cloudinary.com/ritchieng/image/upload/v1474072798/mdp_ritchie_16_09_2016_irve8t.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
            <pubDate>Fri, 16 Sep 2016 00:00:00 +0800</pubDate>
            <link>http://www.ritchieng.com/comic-series/mdp-high-rewards/</link>
            <guid isPermaLink="true">http://www.ritchieng.com/comic-series/mdp-high-rewards/</guid>
            
            <category>comic_series</category>
            
            
        </item>
        
        <item>
            <title>Completed Data School&#39;s Pandas Q&amp;A Series</title>
            <description>&lt;h2 id=&quot;pandas-series-by-data-school&quot;&gt;Pandas Series by Data School&lt;/h2&gt;

&lt;p&gt;This is a free course offered by Data School.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.dataschool.io/easier-data-analysis-with-pandas/&quot;&gt;Introduction to Pandas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;review&quot;&gt;Review&lt;/h2&gt;
&lt;p&gt;If you do not like reading books or guides, this is a very good series. You might think it’s simply a series of videos but it covers a lot of essential parts of pandas that allow you to have an adequate knowledge to work with scikit-learn and actual data sets.&lt;/p&gt;

&lt;p&gt;Again, I cannot emphasise how Mark has a way to deliver simple and succinct explanations on complicated topics. This is very important for beginners.&lt;/p&gt;

&lt;p&gt;I hope you found the &lt;a href=&quot;http://www.ritchieng.com/pandas-introduction/&quot;&gt;notes&lt;/a&gt; that I have derived from the course useful! They are some variations on Github by others so feel free to use them too if you feel their explanations are clearer.&lt;/p&gt;

&lt;p&gt;I will be moving on to Udacity’s Machine Learning nanodegree along with other online resources where I will document my progress. You can view my personal list on how to be a Machine Learning Engineer &lt;a href=&quot;http://www.ritchieng.com/machine-learning-resources/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All the best, Ritchie.&lt;/p&gt;
</description>
            <pubDate>Thu, 18 Aug 2016 00:00:00 +0800</pubDate>
            <link>http://www.ritchieng.com/completed-pandas-qna-data-school/</link>
            <guid isPermaLink="true">http://www.ritchieng.com/completed-pandas-qna-data-school/</guid>
            
            <category>news</category>
            
            <category>machine_learning</category>
            
            <category>becoming_a_machine_learning_engineer</category>
            
            <category>pandas</category>
            
            
        </item>
        
        <item>
            <title>Completed Data School&#39;s free Machine Learning tutorials</title>
            <description>&lt;h2 id=&quot;machine-learning-with-scikit-learn-with-data-school&quot;&gt;Machine Learning with scikit-learn with Data School&lt;/h2&gt;

&lt;p&gt;This is a free course offered by Data School.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.dataschool.io/machine-learning-with-scikit-learn/&quot;&gt;Machine Learning with scikit-learn&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.dataschool.io/easier-data-analysis-with-pandas/&quot;&gt;Introduction to Pandas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;review&quot;&gt;Review&lt;/h2&gt;
&lt;p&gt;The course is good for people who have completed Andrew Ng’s class on Machine Learning. You would start to realise how scikit-learn automates a lot of the low-level Octave code you implemented in Andrew Ng’s class from the simple gradient descent algorithm, to regularization, and the mind-boggling neural networks. Although I will be writing more on Neural Networks using Tensorflow (formerly known as Skflow).&lt;/p&gt;

&lt;p&gt;Also, the tutorials offered by Data School are good for people who are new to scikit-learn and Python in general. If you do not want to watch his free open-source videos, you can check out my notes on all his tutorials.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ritchieng.com/machine-learning-intro-easy/&quot;&gt;Introduction to Machine Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ritchieng.com/ipython-introduction/&quot;&gt;IPython Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ritchieng.com/machine-learning-iris-dataset/&quot;&gt;Exploring Iris Dataset&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ritchieng.com/machine-learning-k-nearest-neighbors-knn/&quot;&gt;KNN Classification Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ritchieng.com/machine-learning-linear-regression/&quot;&gt;Linear Regression Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ritchieng.com/machine-learning-cross-validation/&quot;&gt;Cross-validation for Parameter Tuning, Model Selection and Feature Selection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ritchieng.com/machine-learning-efficiently-search-tuning-param/&quot;&gt;Efficiently Searching Optimal Tuning Parameters&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ritchieng.com/machine-learning-evaluate-classification-model/&quot;&gt;Evaluating a Classification Model&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I hope you found my &lt;a href=&quot;http://www.ritchieng.com/machine-learning-intro-easy/&quot;&gt;notes&lt;/a&gt; useful! They are some variations on Github by others so feel free to use them too if you feel their explanations are clearer.&lt;/p&gt;

&lt;p&gt;I will be moving on &lt;a href=&quot;http://www.dataschool.io/easier-data-analysis-with-pandas/&quot;&gt;Pandas Series by Data School&lt;/a&gt;. Pandas plays an important role in data wrangling. Check out my review on this series soon!&lt;/p&gt;
</description>
            <pubDate>Sat, 30 Jul 2016 00:00:00 +0800</pubDate>
            <link>http://www.ritchieng.com/completed-machine-learning-data-school-start/</link>
            <guid isPermaLink="true">http://www.ritchieng.com/completed-machine-learning-data-school-start/</guid>
            
            <category>news</category>
            
            <category>machine_learning</category>
            
            <category>becoming_a_machine_learning_engineer</category>
            
            
        </item>
        
        <item>
            <title>Completed Machine Learning by Andrew Ng, Stanford University!</title>
            <description>&lt;h2 id=&quot;machine-learning-by-andrew-ng&quot;&gt;Machine Learning by Andrew Ng&lt;/h2&gt;

&lt;p&gt;I completed the course and I have documented my notes under &lt;a href=&quot;http://127.0.0.1:4005/machine-learning/&quot;&gt;Machine Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Meanwhile, you can check out my full Github repository &lt;a href=&quot;https://github.com/ritchieng&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;review&quot;&gt;Review&lt;/h2&gt;

&lt;p&gt;This course teaches you the theoretical foundations of Machine Learning and allows you to apply the theory you learn using Octave (Matlab). I had my doubts about Octave. But because of the simplicity of it, it is indeed a very good choice over many other programming languages.&lt;/p&gt;

&lt;p&gt;The course began and the workload and content were manageable. But halfway through, I almost died.&lt;/p&gt;

&lt;p&gt;The most challenging part of this course is on Neural Networks. The mathematical theory and implementation in code are both very challenging, at least to me. The other parts of the course are much easier than the Neural Networks’ weeks. So do not give up when you are undergoing the weeks teaching Neural Networks. Keep perservering and you will make it through. You can seek help from “Discussions” and with some guides on the tutorials given.&lt;/p&gt;

&lt;p&gt;I learnt a lot from this course and I could finally understand all the articles I read online and go through the famous scikit-learn documentation understanding many of the algorithms that have been nicely implemented and you can use them with a few lines of codes compared to implementing them from the bottom-up using Octave (Matlab), R, Java, Python, C++ or any other programming languages.&lt;/p&gt;

&lt;p&gt;I believe I found a good accompanying course for this course offered by Carnegie Mellon University (CMU) in case you want to cover topics that were not covered by Andrew Ng or delve into the topics he covered from a different perspective. You can access the course &lt;a href=&quot;http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml&quot;&gt;here&lt;/a&gt; from CMU.&lt;/p&gt;

&lt;p&gt;For now, I am going to complete Data School’s introduction to &lt;a href=&quot;http://www.dataschool.io/machine-learning-with-scikit-learn/&quot;&gt;Machine Learning&lt;/a&gt; using scikit-learn and their guide on &lt;a href=&quot;http://www.dataschool.io/easier-data-analysis-with-pandas/&quot;&gt;Pandas&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And from late August, I will be embarking on Udacity’s Machine Learning nanodegree programme where I will offer my detailed review!&lt;/p&gt;

</description>
            <pubDate>Sun, 17 Jul 2016 00:00:00 +0800</pubDate>
            <link>http://www.ritchieng.com/completed-machine-learning-andrew-ng-stanford/</link>
            <guid isPermaLink="true">http://www.ritchieng.com/completed-machine-learning-andrew-ng-stanford/</guid>
            
            <category>news</category>
            
            <category>machine_learning</category>
            
            <category>becoming_a_machine_learning_engineer</category>
            
            
        </item>
        
    </channel>
</rss>
